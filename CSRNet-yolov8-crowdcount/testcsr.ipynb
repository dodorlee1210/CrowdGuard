{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSRNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512, 256, 128, 64]\n",
    "        self.frontend = self.make_layers(self.frontend_feat)\n",
    "        self.backend = self.make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "            self._initialize_weights()\n",
    "            frontend_state_dict = self.frontend.state_dict()\n",
    "            vgg_state_dict = mod.state_dict()\n",
    "            for k in frontend_state_dict.keys():\n",
    "                if k in vgg_state_dict:\n",
    "                    frontend_state_dict[k] = vgg_state_dict[k]\n",
    "            self.frontend.load_state_dict(frontend_state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        layers = []\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Initialize the CSRNet model\n",
    "model = CSRNet()\n",
    "\n",
    "# Load the pretrained CSRNet model weights (assuming you have a checkpoint file named 'PartAmodel_best.pth.tar')\n",
    "checkpoint = torch.load('checkpoint/PartAmodel_best.pth.tar', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize to reduce computation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Path to the input video file\n",
    "input_video_path = \"inputs/simulation.mp4\"\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 5  # Process every 3rd frame\n",
    "\n",
    "# Loop to continuously get frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Finished processing video\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Resize the frame for faster processing\n",
    "    frame_resized = cv2.resize(frame, (512, 512))\n",
    "\n",
    "    # Preprocess the frame\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    input_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run the CSRNet model on the frame\n",
    "    with torch.no_grad():\n",
    "        density_map = model(input_tensor)\n",
    "\n",
    "    # Convert density map to count\n",
    "    density_map_np = density_map.squeeze().cpu().numpy()\n",
    "    count = np.sum(density_map_np)\n",
    "\n",
    "    # Display the count on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    cv2.putText(annotated_frame, f\"Count: {int(count)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Crowd Counting\", annotated_frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSRNET + density map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512, 256, 128, 64]\n",
    "        self.frontend = self.make_layers(self.frontend_feat)\n",
    "        self.backend = self.make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "            self._initialize_weights()\n",
    "            frontend_state_dict = self.frontend.state_dict()\n",
    "            vgg_state_dict = mod.state_dict()\n",
    "            for k in frontend_state_dict.keys():\n",
    "                if k in vgg_state_dict:\n",
    "                    frontend_state_dict[k] = vgg_state_dict[k]\n",
    "            self.frontend.load_state_dict(frontend_state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        layers = []\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Initialize the CSRNet model\n",
    "model = CSRNet()\n",
    "\n",
    "# Load the pretrained CSRNet model weights (assuming you have a checkpoint file named 'PartAmodel_best.pth.tar')\n",
    "checkpoint = torch.load('checkpoint/PartAmodel_best.pth.tar', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((654, 654)),  # Resize to reduce computation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Path to the input video file\n",
    "input_video_path = \"inputs/simulation.mp4\"\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 5  # Process every 5th frame\n",
    "\n",
    "# Loop to continuously get frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Finished processing video\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Resize the frame for faster processing\n",
    "    frame_resized = cv2.resize(frame, (512, 512))\n",
    "\n",
    "    # Preprocess the frame\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    input_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run the CSRNet model on the frame\n",
    "    with torch.no_grad():\n",
    "        density_map = model(input_tensor)\n",
    "\n",
    "    # Convert density map to count\n",
    "    density_map_np = density_map.squeeze().cpu().numpy()\n",
    "    count = np.sum(density_map_np)\n",
    "\n",
    "    # Normalize density map for visualization\n",
    "    density_map_np = density_map_np / np.max(density_map_np)  # Normalize between 0 and 1\n",
    "    density_map_np = (density_map_np * 255).astype(np.uint8)  # Scale to 255 for display\n",
    "\n",
    "    # Apply a color map to the density map\n",
    "    density_map_colored = cv2.applyColorMap(density_map_np, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Resize density map to match original frame size\n",
    "    density_map_resized = cv2.resize(density_map_colored, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Display the count on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    cv2.putText(annotated_frame, f\"Count: {int(count)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Concatenate the frame and the density map for side-by-side display\n",
    "    combined_display = cv2.hconcat([annotated_frame, density_map_resized])\n",
    "\n",
    "    # Display the annotated frame and the density map\n",
    "    cv2.imshow(\"Crowd Counting and Density Map\", combined_display)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSRNET + density + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512, 256, 128, 64]\n",
    "        self.frontend = self.make_layers(self.frontend_feat)\n",
    "        self.backend = self.make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "            self._initialize_weights()\n",
    "            frontend_state_dict = self.frontend.state_dict()\n",
    "            vgg_state_dict = mod.state_dict()\n",
    "            for k in frontend_state_dict.keys():\n",
    "                if k in vgg_state_dict:\n",
    "                    frontend_state_dict[k] = vgg_state_dict[k]\n",
    "            self.frontend.load_state_dict(frontend_state_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "        layers = []\n",
    "        if dilation:\n",
    "            d_rate = 2\n",
    "        else:\n",
    "            d_rate = 1\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Initialize the CSRNet model\n",
    "model = CSRNet()\n",
    "\n",
    "# Load the pretrained CSRNet model weights (assuming you have a checkpoint file named 'PartAmodel_best.pth.tar')\n",
    "checkpoint = torch.load('checkpoint/PartAmodel_best.pth.tar', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((654, 654)),  # Resize to reduce computation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Path to the input video file\n",
    "input_video_path = \"inputs/simulation.mp4\"\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "frame_count = 0\n",
    "frame_skip = 5  # Process every 5th frame\n",
    "\n",
    "# Loop to continuously get frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Finished processing video\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Resize the frame for faster processing\n",
    "    frame_resized = cv2.resize(frame, (512, 512))\n",
    "\n",
    "    # Preprocess the frame\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    input_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run the CSRNet model on the frame\n",
    "    with torch.no_grad():\n",
    "        density_map = model(input_tensor)\n",
    "\n",
    "    # Convert density map to count\n",
    "    density_map_np = density_map.squeeze().cpu().numpy()\n",
    "    count = np.sum(density_map_np)\n",
    "\n",
    "    # Normalize density map for visualization\n",
    "    density_map_np_normalized = density_map_np / np.max(density_map_np)  # Normalize between 0 and 1\n",
    "    density_map_np_visual = (density_map_np_normalized * 255).astype(np.uint8)  # Scale to 255 for display\n",
    "\n",
    "    # Apply a color map to the density map\n",
    "    density_map_colored = cv2.applyColorMap(density_map_np_visual, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Resize density map to match original frame size\n",
    "    density_map_resized = cv2.resize(density_map_colored, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Highlight areas of high density\n",
    "    threshold = 0.9  # Adjust this threshold based on what is considered a high density\n",
    "    high_density_mask = (density_map_np_normalized > threshold).astype(np.uint8) * 255\n",
    "    high_density_colored = cv2.applyColorMap(high_density_mask, cv2.COLORMAP_HOT)\n",
    "    high_density_mask_resized = cv2.resize(high_density_mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Overlay high-density areas on the original frame\n",
    "    high_density_colored_resized = cv2.resize(high_density_colored, (frame.shape[1], frame.shape[0]))\n",
    "    frame[high_density_mask_resized > 0] = high_density_colored_resized[high_density_mask_resized > 0]\n",
    "\n",
    "    # Display the count on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    cv2.putText(annotated_frame, f\"Count: {int(count)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Concatenate the frame and the density map for side-by-side display\n",
    "    combined_display = cv2.hconcat([annotated_frame, density_map_resized])\n",
    "\n",
    "    # Display the annotated frame and the density map\n",
    "    cv2.imshow(\"Crowd Counting and Density Map\", combined_display)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 14 persons, 18.4ms\n",
      "Speed: 4.3ms preprocess, 18.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 16.2ms\n",
      "Speed: 3.2ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 17.2ms\n",
      "Speed: 2.8ms preprocess, 17.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 16.1ms\n",
      "Speed: 3.8ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.6ms\n",
      "Speed: 4.0ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.7ms\n",
      "Speed: 3.2ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.6ms\n",
      "Speed: 2.6ms preprocess, 15.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.6ms\n",
      "Speed: 2.8ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.7ms\n",
      "Speed: 3.7ms preprocess, 15.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.5ms\n",
      "Speed: 2.8ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 4.6ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.2ms\n",
      "Speed: 2.8ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.3ms\n",
      "Speed: 3.1ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 17.3ms\n",
      "Speed: 3.5ms preprocess, 17.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 3.3ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 34.4ms\n",
      "Speed: 7.5ms preprocess, 34.4ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 19.7ms\n",
      "Speed: 5.6ms preprocess, 19.7ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 20.7ms\n",
      "Speed: 6.7ms preprocess, 20.7ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 20.2ms\n",
      "Speed: 6.3ms preprocess, 20.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 20.8ms\n",
      "Speed: 6.1ms preprocess, 20.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 19.2ms\n",
      "Speed: 7.5ms preprocess, 19.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 17.4ms\n",
      "Speed: 6.6ms preprocess, 17.4ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 22.3ms\n",
      "Speed: 4.9ms preprocess, 22.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 24.7ms\n",
      "Speed: 5.7ms preprocess, 24.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 19.0ms\n",
      "Speed: 5.9ms preprocess, 19.0ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.7ms\n",
      "Speed: 4.6ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.4ms\n",
      "Speed: 5.4ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 4.4ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.2ms\n",
      "Speed: 2.6ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.5ms\n",
      "Speed: 3.1ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.4ms\n",
      "Speed: 4.0ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.3ms\n",
      "Speed: 3.4ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 16.2ms\n",
      "Speed: 3.9ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 3.0ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 16.3ms\n",
      "Speed: 2.4ms preprocess, 16.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 3.1ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.4ms\n",
      "Speed: 2.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.4ms\n",
      "Speed: 3.0ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 16 persons, 1 kite, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 3.6ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 16.3ms\n",
      "Speed: 2.6ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 3.7ms preprocess, 15.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.4ms\n",
      "Speed: 5.9ms preprocess, 15.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.4ms\n",
      "Speed: 3.8ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.5ms\n",
      "Speed: 6.7ms preprocess, 15.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.4ms\n",
      "Speed: 3.0ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.5ms\n",
      "Speed: 3.1ms preprocess, 15.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.8ms preprocess, 15.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 3.6ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.4ms\n",
      "Speed: 6.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.4ms\n",
      "Speed: 2.6ms preprocess, 15.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.4ms\n",
      "Speed: 3.2ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.4ms\n",
      "Speed: 2.7ms preprocess, 15.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.4ms\n",
      "Speed: 2.4ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 19.5ms\n",
      "Speed: 5.3ms preprocess, 19.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.5ms\n",
      "Speed: 4.3ms preprocess, 15.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.4ms\n",
      "Speed: 2.6ms preprocess, 15.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.4ms\n",
      "Speed: 2.8ms preprocess, 15.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 16.3ms\n",
      "Speed: 3.3ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 17.5ms\n",
      "Speed: 4.1ms preprocess, 17.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 16 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.4ms\n",
      "Speed: 3.9ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 21 persons, 15.2ms\n",
      "Speed: 2.8ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.4ms\n",
      "Speed: 3.3ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 16 persons, 17.5ms\n",
      "Speed: 3.0ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 16 persons, 15.3ms\n",
      "Speed: 2.8ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.4ms\n",
      "Speed: 3.6ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 15 persons, 15.2ms\n",
      "Speed: 2.2ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 persons, 16.3ms\n",
      "Speed: 2.7ms preprocess, 16.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 23 persons, 15.4ms\n",
      "Speed: 3.1ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 persons, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 persons, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.3ms\n",
      "Speed: 2.7ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.2ms\n",
      "Speed: 3.0ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 22 persons, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.3ms\n",
      "Speed: 2.7ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 19 persons, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 28 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 22 persons, 16.2ms\n",
      "Speed: 3.2ms preprocess, 16.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 persons, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 22 persons, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 22 persons, 15.3ms\n",
      "Speed: 2.7ms preprocess, 15.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 26 persons, 15.4ms\n",
      "Speed: 2.4ms preprocess, 15.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 20 persons, 16.3ms\n",
      "Speed: 2.5ms preprocess, 16.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 21 persons, 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 15.5ms\n",
      "Speed: 2.7ms preprocess, 15.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 18 persons, 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 16.2ms\n",
      "Speed: 3.6ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 11 persons, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 10 persons, 15.3ms\n",
      "Speed: 3.1ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 12 persons, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 14 persons, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 11 persons, 15.3ms\n",
      "Speed: 2.7ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 13 persons, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 9 persons, 16.4ms\n",
      "Speed: 2.4ms preprocess, 16.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # or any other YOLOv8 model\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(\"inputs/simulation.mp4\")\n",
    "\n",
    "# Loop through the frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run the YOLOv8 model on the frame\n",
    "    results = model(frame, conf = 0.1)\n",
    "\n",
    "    # Extract detections\n",
    "    detections = results[0].boxes.data.cpu().numpy()  # Get the bounding box detections\n",
    "\n",
    "    people_count = 0\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        if int(cls) == 0:  # Assuming 'person' class is labeled as 0\n",
    "            people_count += 1\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the count of people on the frame\n",
    "    cv2.putText(frame, f'People Count: {people_count}', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Press 'q' to exit the video\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
